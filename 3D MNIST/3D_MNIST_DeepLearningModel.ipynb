{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/3d-mnist/voxelgrid.py\n/kaggle/input/3d-mnist/test_point_clouds.h5\n/kaggle/input/3d-mnist/plot3D.py\n/kaggle/input/3d-mnist/full_dataset_vectors.h5\n/kaggle/input/3d-mnist/train_point_clouds.h5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 3D MNIST"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\nimport h5py\n\n%matplotlib inline","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with h5py.File('../input/3d-mnist/full_dataset_vectors.h5', 'r') as hf:\n    x_train_ = hf[\"X_train\"][:]\n    y_train_ = hf[\"y_train\"][:]\n    x_test_ = hf[\"X_test\"][:]\n    y_test_ = hf[\"y_test\"][:]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1D vector to rgb values, provided by ../input/plot3d.py\ndef array_to_color(array, cmap=\"Oranges\"):\n    s_m = plt.cm.ScalarMappable(cmap=cmap)\n    return s_m.to_rgba(array)[:,:-1]\n\n# Transform data from 1d to 3d rgb\ndef rgb_data_transform(data):\n    data_t = []\n    for i in range(data.shape[0]):\n        data_t.append(array_to_color(data[i]).reshape(16, 16, 16, 3))\n    return np.asarray(data_t, dtype=np.float32)\nn_classes = 10 # from 0 to 9, 10 labels totally\n\nx_train = rgb_data_transform(x_train_)\nx_test = rgb_data_transform(x_test_)\n\ny_train = to_categorical(y_train_, n_classes)\ny_test = to_categorical(y_test_, n_classes)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Counting the Zero(0) to Nine(9)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nTrain')\nunique_elements, counts_elements = np.unique(y_train_, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))\n\nprint('\\nTest')\nunique_elements, counts_elements = np.unique(y_test_, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))","execution_count":25,"outputs":[{"output_type":"stream","text":"\nTrain\n[[   0    1    2    3    4    5    6    7    8    9]\n [ 958 1126  976  986 1070  868 1002 1100  924  990]]\n\nTest\n[[  0   1   2   3   4   5   6   7   8   9]\n [170 252 232 214 220 174 174 198 178 188]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv3D, MaxPool3D, Dense, Flatten, Dropout, BatchNormalization, Activation\nfrom keras.optimizers import Adadelta, Adam, RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Callbacks\nModelCheckpoint -> Save best model.  \nEarlyStopping -> If not improve reference value, model stop.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath=\"best_model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(patience = 10,monitor='val_loss', verbose=0, mode='min')\ncallbacks_list = [checkpoint, early_stopping]","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple NN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"models= Sequential()\nmodels.add(Dense(256, input_shape=(4096,)))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.25))\n\nmodels.add(Dense(256))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.25))\n\nmodels.add(Dense(256))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.25))\n\nmodels.add(Dense(128))\nmodels.add(BatchNormalization())\nmodels.add(Activation('elu'))\nmodels.add(Dropout(0.5))\n\nmodels.add(Dense(10, activation='softmax'))","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0008), metrics=['acc'])\nmodels.fit(x_train_, y_train_, batch_size=32, epochs=150, validation_split=0.2, verbose=2,\\\n          callbacks=callbacks_list)","execution_count":41,"outputs":[{"output_type":"stream","text":"Train on 8000 samples, validate on 2000 samples\nEpoch 1/150\n - 5s - loss: 1.8841 - acc: 0.4098 - val_loss: 1.2886 - val_acc: 0.5765\n\nEpoch 00001: val_loss improved from inf to 1.28863, saving model to best_model.hdf5\nEpoch 2/150\n - 3s - loss: 1.3725 - acc: 0.5523 - val_loss: 1.2157 - val_acc: 0.5890\n\nEpoch 00002: val_loss improved from 1.28863 to 1.21570, saving model to best_model.hdf5\nEpoch 3/150\n - 3s - loss: 1.2080 - acc: 0.5957 - val_loss: 1.1678 - val_acc: 0.6060\n\nEpoch 00003: val_loss improved from 1.21570 to 1.16783, saving model to best_model.hdf5\nEpoch 4/150\n - 3s - loss: 1.1239 - acc: 0.6237 - val_loss: 1.1166 - val_acc: 0.6145\n\nEpoch 00004: val_loss improved from 1.16783 to 1.11655, saving model to best_model.hdf5\nEpoch 5/150\n - 4s - loss: 1.0174 - acc: 0.6551 - val_loss: 1.1345 - val_acc: 0.6180\n\nEpoch 00005: val_loss did not improve from 1.11655\nEpoch 6/150\n - 4s - loss: 0.9891 - acc: 0.6680 - val_loss: 1.1827 - val_acc: 0.6130\n\nEpoch 00006: val_loss did not improve from 1.11655\nEpoch 7/150\n - 4s - loss: 0.9051 - acc: 0.6981 - val_loss: 1.1628 - val_acc: 0.6170\n\nEpoch 00007: val_loss did not improve from 1.11655\nEpoch 8/150\n - 4s - loss: 0.8464 - acc: 0.7199 - val_loss: 1.1628 - val_acc: 0.6250\n\nEpoch 00008: val_loss did not improve from 1.11655\nEpoch 9/150\n - 4s - loss: 0.8128 - acc: 0.7289 - val_loss: 1.1760 - val_acc: 0.6250\n\nEpoch 00009: val_loss did not improve from 1.11655\nEpoch 10/150\n - 3s - loss: 0.7602 - acc: 0.7494 - val_loss: 1.2489 - val_acc: 0.6265\n\nEpoch 00010: val_loss did not improve from 1.11655\nEpoch 11/150\n - 3s - loss: 0.7093 - acc: 0.7595 - val_loss: 1.2080 - val_acc: 0.6240\n\nEpoch 00011: val_loss did not improve from 1.11655\nEpoch 12/150\n - 3s - loss: 0.6653 - acc: 0.7766 - val_loss: 1.1971 - val_acc: 0.6390\n\nEpoch 00012: val_loss did not improve from 1.11655\nEpoch 13/150\n - 3s - loss: 0.6362 - acc: 0.7889 - val_loss: 1.2632 - val_acc: 0.6255\n\nEpoch 00013: val_loss did not improve from 1.11655\nEpoch 14/150\n - 3s - loss: 0.5961 - acc: 0.8052 - val_loss: 1.2859 - val_acc: 0.6250\n\nEpoch 00014: val_loss did not improve from 1.11655\n","name":"stdout"},{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f049c4a5f60>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.evaluate(x_test_,y_test_,batch_size=32)","execution_count":42,"outputs":[{"output_type":"stream","text":"2000/2000 [==============================] - 0s 107us/step\n","name":"stdout"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"[1.2804147634506227, 0.6269999742507935]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In modelling, test accurracy approach 0.8. Validation accuracy approch about 0.63. This model is overfitting. I found loss and validation loss is big difference and it is overfitting. I tried add dropout layer and change dropout ratio. But it is difficulty to prevent overfitting.\n\nLoss is about 1.280 and Accuracy is about 62.6%."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Conv3D Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv3D(32,(3,3,3), input_shape=(16,16,16,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Conv3D(64,(3,3,3), input_shape=(16,16,16,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(MaxPool3D(pool_size=(2, 2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv3D(32,(2,2,2), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(64))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(64))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0008), metrics=['acc'])\nhist=model.fit(x_train, y_train, batch_size=128, epochs=80, validation_split=0.2, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(x_test,y_test,batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In modelling, test accurracy approach about 0.97. Validation accuracy is almost 0.69. However, this is not accruate because of valiation loss is not stable and very fluctuated. This model has also overfitting problem. I tried add dropout layer and change dropout ratio and Batchnormalization. It is difficulty to prevent overfitting. \n\nLoss is about 1.6 and Accuracy is about 69.1%."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Plot Loss and Valid_Loss\n# 2. Plot Accuracy and Valid_Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1, figsize=(12, 12))\nax[0].plot((loss), 'bo', label=\"Loss\")\nax[0].plot((val_loss), 'b', label=\"Valid_Loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss')\n\nax[1].plot((acc), 'bo', label=\"Accuracy\")\nax[1].plot((val_acc), 'b',label=\"Valid_Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}