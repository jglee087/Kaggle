{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploartory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data= pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum() # checking for total null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age has 177 null data.  \n",
    "Cabin has 687 null data.  \n",
    "Embarked has 2 null data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Survived??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "data['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Survived')\n",
    "ax[0].set_ylabel('')\n",
    "\n",
    "sns.countplot('Survived',data=data,ax=ax[1])\n",
    "ax[1].set_title('Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n",
    "We will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.\n",
    "First let us understand the different types of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types Of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Features(범주형 데이터)**\n",
    "\n",
    "범주형 변수는 두 개 이상의 범주가 있는 변수이며 해당 기능의 각 값을 범주별로 분류 할 수 있다.\n",
    "\n",
    "A categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables.  \n",
    "\n",
    "**Categorical Features in the dataset: Sex,Embarked.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal Features(순서 자료)**  \n",
    "\n",
    "순서 변수는 범주형 데이터와 비슷하지만 차이점은 값 사이의 상대적인 정렬이 가능하다. 순서가 있다는 의미이다. 예를 들면, tall, medium, short가 있다면 이는 각 값 자체가 순서를 가질 수 있다.  \n",
    "\n",
    "An ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort in the variable.  \n",
    "\n",
    "**Ordinal Features in the dataset: PClass**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continous Feature(연속 자료)**  \n",
    "\n",
    "열의 임의의 두 값을 표현하는데  최소 값과 최대 값 사이의 값을 사용할 수있는 경우 연속형 자료라고 할 수 있다.\n",
    "\n",
    "A feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n",
    "\n",
    "**Continous Features in the dataset: Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing The Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex--> Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby 함수\n",
    "groupby 함수는 pandas에서 특정 열 or 열들을 기준으로 정렬하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.groupby(['Sex','Survived'])['Survived']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Survived vs Sex')\n",
    "sns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\n",
    "ax[1].set_title('Sex:Survived vs Dead')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 is dead, 1 is alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pclass --> Ordinal Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.Pclass,data.Survived,margins=True).\\\n",
    "            style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "data['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\n",
    "ax[0].set_title('Number Of Passengers By Pclass')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "sns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\n",
    "ax[1].set_title('Pclass:Survived vs Dead')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around **25%.**  \n",
    "For Pclass 1 %survived is around **63%** while for Pclass2 is around **48%.** So money and status matters. Such a materialistic world.  \n",
    "Lets Dive in little bit more and check for other interesting observations. Lets check survival rate with Sex and Pclass Together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).\\\n",
    "            style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Pclass','Survived','Sex'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Pclass','Survived',hue='Sex',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " FactorPlot in this case, because they make the seperation of categorical values easy.  \n",
    "Looking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age--> Continous Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Oldest Passenger was of:',data['Age'].max(),'Years')\n",
    "print('Youngest Passenger was of:',data['Age'].min(),'Years')\n",
    "print('Average Age on the ship:',data['Age'].mean(),'Years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hue: 색조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "sns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])\n",
    "ax[0].set_title('Pclass and Age vs Survived')\n",
    "ax[0].set_yticks(range(0,110,10))\n",
    "\n",
    "sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\n",
    "ax[1].set_title('Sex and Age vs Survived')\n",
    "ax[1].set_yticks(range(0,110,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "1) The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.  \n",
    "2) Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.  \n",
    "3) For males, the survival chances decreases with an increase in age.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset.  \n",
    "\n",
    "But the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie??\n",
    "\n",
    "Bingo!!!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Initial']=0\n",
    "test_data['Initial']=0\n",
    "for i in data:\n",
    "    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "    test_data['Initial']=test_data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the Regex: [A-Za-z]+)\\\\.  \n",
    "So what it does is, it looks for strings which lie between A-Z or a-z and followed by a **.(dot)**. So we successfully extract the Initials from the Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\\\n",
    "                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n",
    "\n",
    "test_data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\\\n",
    "                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Mr'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Initial')['Age'].mean() #lets check the average age by Initials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling NaN Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mr'),'Age']=33\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Mrs'),'Age']=36\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Master'),'Age']=5\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Miss'),'Age']=22\n",
    "test_data.loc[(test_data.Age.isnull())&(test_data.Initial=='Other'),'Age']=46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Age.isnull().any(), test_data.Age.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "data[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\n",
    "ax[0].set_title('Survived= 0')\n",
    "x1=list(range(0,85,5))\n",
    "ax[0].set_xticks(x1)\n",
    "\n",
    "data[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\n",
    "ax[1].set_title('Survived= 1')\n",
    "x2=list(range(0,85,5))\n",
    "ax[1].set_xticks(x2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Pclass','Survived',col='Initial',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked--> Categorical Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Embarked','Survived',data=data)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(5,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,2,figsize=(20,15))\n",
    "sns.countplot('Embarked',data=data,ax=ax[0,0])\n",
    "ax[0,0].set_title('No. Of Passengers Boarded')\n",
    "sns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\n",
    "ax[0,1].set_title('Male-Female Split for Embarked')\n",
    "sns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\n",
    "ax[1,0].set_title('Embarked vs Survived')\n",
    "sns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\n",
    "ax[1,1].set_title('Embarked vs Pclass')\n",
    "plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling Embarked NaN**  \n",
    "As we saw that maximum passengers boarded from Port S, we replace NaN with S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Embarked.isnull().any()# Finally No NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SibSip-->Discrete Feature\n",
    "This feature represents whether a person is alone or with his family members.  \n",
    "Sibling = brother, sister, stepbrother, stepsister  \n",
    "Spouse = husband, wife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "\n",
    "sns.barplot('SibSp','Survived',data=data,ax=ax[0])\n",
    "ax[0].set_title('SibSp vs Survived')\n",
    "\n",
    "sns.factorplot('SibSp','Survived',data=data,ax=ax[1])\n",
    "ax[1].set_title('SibSp vs Survived')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2,figsize=(20,8))\n",
    "sns.barplot('Parch','Survived',data=data,ax=ax[0])\n",
    "ax[0].set_title('Parch vs Survived')\n",
    "sns.factorplot('Parch','Survived',data=data,ax=ax[1])\n",
    "ax[1].set_title('Parch vs Survived')\n",
    "plt.close(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare--> Continous Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Highest Fare was:',data['Fare'].max())\n",
    "print('Lowest Fare was:',data['Fare'].min())\n",
    "print('Average Fare was:',data['Fare'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,3,figsize=(20,8))\n",
    "\n",
    "sns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\n",
    "ax[0].set_title('Fares in Pclass 1')\n",
    "\n",
    "sns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\n",
    "ax[1].set_title('Fares in Pclass 2')\n",
    "\n",
    "sns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\n",
    "ax[2].set_title('Fares in Pclass 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Between The Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting The Heatmap\n",
    "The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.  \n",
    "\n",
    "POSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.  \n",
    "\n",
    "NEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.  \n",
    "\n",
    "Now lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.  \n",
    "\n",
    "So do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.  \n",
    "\n",
    "Now from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Feature Engineering and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now what is Feature Engineering?**  \n",
    "\n",
    "Whenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.  \n",
    "\n",
    "An example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age_band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem With Age Feature:\n",
    "As I have mentioned earlier that Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.  \n",
    "\n",
    "Eg:If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.  \n",
    "\n",
    "Now if I say to group them by their Age, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.  \n",
    "\n",
    "We need to convert these continous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.  \n",
    "\n",
    "Okay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16. So bins of size 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_band']=0\n",
    "data.loc[data['Age']<=16,'Age_band']=0\n",
    "data.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\n",
    "data.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\n",
    "data.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\n",
    "data.loc[data['Age']>64,'Age_band']=4\n",
    "\n",
    "test_data['Age_band']=0\n",
    "test_data.loc[test_data['Age']<=16,'Age_band']=0\n",
    "test_data.loc[(test_data['Age']>16)&(test_data['Age']<=32),'Age_band']=1\n",
    "test_data.loc[(test_data['Age']>32)&(test_data['Age']<=48),'Age_band']=2\n",
    "test_data.loc[(test_data['Age']>48)&(test_data['Age']<=64),'Age_band']=3\n",
    "test_data.loc[test_data['Age']>64,'Age_band']=4\n",
    "\n",
    "data.head(2)\n",
    "#test_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Age_band','Survived',data=data,col='Pclass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family_Size and Alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Family_Size']=0\n",
    "data['Family_Size']=data['Parch']+data['SibSp']#family size\n",
    "data['Alone']=0\n",
    "data.loc[data.Family_Size==0,'Alone']=1 #Alone\n",
    "\n",
    "f,ax=plt.subplots(1,2,figsize=(18,6))\n",
    "sns.factorplot('Family_Size','Survived',data=data,ax=ax[0])\n",
    "ax[0].set_title('Family_Size vs Survived')\n",
    "sns.factorplot('Alone','Survived',data=data,ax=ax[1])\n",
    "ax[1].set_title('Alone vs Survived')\n",
    "plt.close(2)\n",
    "plt.close(3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Family_Size']=0\n",
    "test_data['Family_Size']=test_data['Parch']+test_data['SibSp']#family size\n",
    "test_data['Alone']=0\n",
    "test_data.loc[data.Family_Size==0,'Alone']=1 #Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare_Range\n",
    "Since fare is also a continous feature, we need to convert it into ordinal value. For this we will use pandas.qcut.\n",
    "So what qcut does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fare_Range']=pd.qcut(data['Fare'],4)\n",
    "data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fare_cat']=0\n",
    "data.loc[data['Fare']<=7.91,'Fare_cat']=0\n",
    "data.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\n",
    "data.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\n",
    "data.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3\n",
    "\n",
    "test_data['Fare_cat']=0\n",
    "test_data.loc[test_data['Fare']<=7.91,'Fare_cat']=0\n",
    "test_data.loc[(test_data['Fare']>7.91)&(test_data['Fare']<=14.454),'Fare_cat']=1\n",
    "test_data.loc[(test_data['Fare']>14.454)&(test_data['Fare']<=31),'Fare_cat']=2\n",
    "test_data.loc[(test_data['Fare']>31)&(test_data['Fare']<=513),'Fare_cat']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting String Values into Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n",
    "data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)\n",
    "\n",
    "test_data['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "test_data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n",
    "test_data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping UnNeeded Features\n",
    "Name--> We don't need name feature as it cannot be converted into any categorical value.  \n",
    "Age--> We have the Age_band feature, so no need of this.  \n",
    "Ticket--> It is any random string that cannot be categorised.  \n",
    "Fare--> We have the Fare_cat feature, so unneeded  \n",
    "Cabin--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.  \n",
    "Fare_Range--> We have the fare_cat feature.  \n",
    "PassengerId--> Cannot be categorised.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "test_data.drop(['Name','Age','Ticket','Fare','Cabin'],axis=1,inplace=True)\n",
    "\n",
    "sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(18,15)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Predict with Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:  \n",
    "1) Logistic Regression  \n",
    "2) Support Vector Machines(Linear and radial)  \n",
    "3) Random Forest  \n",
    "4) K-Nearest Neighbours  \n",
    "5) Naive Bayes  \n",
    "6) Decision Tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\n",
    "train_X=train[train.columns[1:]]\n",
    "train_Y=train[train.columns[:1]]\n",
    "test_X=test[test.columns[1:]]\n",
    "test_Y=test[test.columns[:1]]\n",
    "X=data[data.columns[1:]]\n",
    "Y=data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y=test_data['PassengerId']\n",
    "del test_data['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps=Input(shape=(10,))\n",
    "x=Dense(16)(inps)\n",
    "x=Activation('relu')(x)\n",
    "x=Dense(16)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Dense(16)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Dense(16)(x)\n",
    "x=Activation('relu')(x)\n",
    "outs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inps, outputs=outs)\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=8.e-5),metrics=['accuracy'], )\n",
    "model.fit(train_X,train_Y,batch_size=4, epochs=150, verbose=2, \\\n",
    "          validation_data=[test_X,test_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=model.predict(test_data)\n",
    "pred_test=pred_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test[pred_test>0.5]=1\n",
    "pred_test[pred_test<0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=pred_test.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Survived = pd.Series(pred_test, name=\"Survived\")\n",
    "results = pd.concat([test_y,test_Survived],axis=1)\n",
    "results.to_csv(\"titanic_dnn_predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model for StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skfold=6\n",
    "skf =StratifiedKFold(n_splits=skfold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps=Input(shape=(10,))\n",
    "\n",
    "for enum, (tr_idx, val_idx) in enumerate(skf.split(X,Y)):\n",
    "    \n",
    "    check_point = ModelCheckpoint(f'./bestmodel_{enum}.hdf5',monitor='val_loss',verbose=1,\\\n",
    "                                  save_best_only=True, mode='min')\n",
    "    k_x_train=X.iloc[tr_idx]\n",
    "    k_y_train=Y.iloc[tr_idx]\n",
    "    k_x_val=X.iloc[val_idx]\n",
    "    k_y_val=Y.iloc[val_idx]\n",
    "    \n",
    "    x=Dense(16)(inps)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dense(16)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dense(16)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dense(16)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    outs=Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inps, outputs=outs)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=8.e-5),metrics=['accuracy'], )\n",
    "    model.fit(k_x_train,k_y_train,batch_size=4, epochs=200, verbose=0, \\\n",
    "              validation_data=[k_x_val,k_y_val],callbacks=[check_point])\n",
    "\n",
    "    print(model.evaluate(test_X,test_Y,batch_size=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "pred_test=np.zeros((418,1))\n",
    "\n",
    "preds=[]\n",
    "for i in range(5):\n",
    "    model=load_model(\"./model_{}.hdf5\".format(i))\n",
    "    pred = np.array(model.predict(test_data))\n",
    "    pred_test += pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred=pred_test/skfold\n",
    "pred[pred>0.5]=1\n",
    "pred[pred<0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.astype('int64')\n",
    "pred = pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Survived = pd.Series(pred, name=\"Survived\")\n",
    "results = pd.concat([test_y,test_Survived],axis=1)\n",
    "results.to_csv(\"titanic_dnn_fold_predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Support Vector Machines(rbf-SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction1=model.predict(test_X)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction00=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction00.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.shape #pred_test.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(418):\n",
    "    print(prediction00[i],pred_test[i].astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=pred_test.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Survived = pd.Series(pred_test, name=\"Survived\")\n",
    "results = pd.concat([test_y,test_Survived],axis=1)\n",
    "results.to_csv(\"titanic_dnn_predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine(linear-SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction2=model.predict(test_X)\n",
    "print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_X,train_Y)\n",
    "prediction3=model.predict(test_X)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier()\n",
    "model.fit(train_X,train_Y)\n",
    "prediction4=model.predict(test_X)\n",
    "print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KNeighborsClassifier() \n",
    "model.fit(train_X,train_Y)\n",
    "prediction5=model.predict(test_X)\n",
    "print('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_index=list(range(1,21))\n",
    "a=pd.Series()\n",
    "#x=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,]\n",
    "x=a_index\n",
    "for i in list(range(1,21)):\n",
    "    model=KNeighborsClassifier(n_neighbors=i) \n",
    "    model.fit(train_X,train_Y)\n",
    "    prediction=model.predict(test_X)       \n",
    "    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\n",
    "plt.plot(a_index, a)\n",
    "plt.xticks(x)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(12,6)\n",
    "plt.show()\n",
    "print('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GaussianNB()\n",
    "model.fit(train_X,train_Y)\n",
    "prediction6=model.predict(test_X)\n",
    "print('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction7=model.predict(test_X)\n",
    "print('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.  \n",
    "\n",
    "1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.  \n",
    "\n",
    "2)Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.  \n",
    "\n",
    "3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.  \n",
    "\n",
    "This is called K-Fold Cross Validation.  \n",
    "\n",
    "4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "\n",
    "xyz=[]\n",
    "accuracy=[]\n",
    "std=[]\n",
    "classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\n",
    "models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result=cv_result\n",
    "    xyz.append(cv_result.mean())\n",
    "    std.append(cv_result.std())\n",
    "    accuracy.append(cv_result)\n",
    "\n",
    "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \n",
    "new_models_dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "box=pd.DataFrame(accuracy,index=[classifiers])\n",
    "box.T.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\n",
    "plt.title('Average CV Mean Accuracy')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "It gives the number of correct and incorrect classifications made by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(3,3,figsize=(12,10))\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\n",
    "ax[0,0].set_title('Matrix for rbf-SVM')\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\n",
    "ax[0,1].set_title('Matrix for Linear-SVM')\n",
    "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\n",
    "ax[0,2].set_title('Matrix for KNN')\n",
    "y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\n",
    "ax[1,0].set_title('Matrix for Random-Forests')\n",
    "y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\n",
    "ax[1,1].set_title('Matrix for Logistic Regression')\n",
    "y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\n",
    "ax[1,2].set_title('Matrix for Decision Tree')\n",
    "y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\n",
    "ax[2,0].set_title('Matrix for Naive Bayes')\n",
    "plt.subplots_adjust(hspace=0.2,wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters Tuning  \n",
    "\n",
    "The machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.  \n",
    "We will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "kernel=['rbf','linear']\n",
    "hyper={'kernel':kernel,'C':C,'gamma':gamma}\n",
    "gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=range(100,1000,100)\n",
    "hyper={'n_estimators':n_estimators}\n",
    "gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling  \n",
    "Ensembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.  \n",
    "\n",
    "Lets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is Ensembling, which improves the stability of the model. Ensembling can be done in ways like:  \n",
    "\n",
    "1)Voting Classifier  \n",
    "2)Bagging  \n",
    "3)Boosting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "It is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n",
    "                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n",
    "                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n",
    "                                              ('LR',LogisticRegression(C=0.05)),\n",
    "                                              ('DT',DecisionTreeClassifier(random_state=0)),\n",
    "                                              ('NB',GaussianNB()),\n",
    "                                              ('svm',svm.SVC(kernel='linear',probability=True))\n",
    "                                             ], \n",
    "                       voting='soft').fit(train_X,train_Y)\n",
    "print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\n",
    "cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\n",
    "print('The cross validated score is',cross.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Bagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged KNN\n",
    "Bagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "model=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction=model.predict(test_X)\n",
    "print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\n",
    "result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for bagged KNN is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction=model.predict(test_X)\n",
    "print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\n",
    "result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for bagged Decision Tree is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "Boosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\n",
    "A model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost(Adaptive Boosting)\n",
    "The weak learner or estimator in this case is a Decsion Tree. But we can change the dafault base_estimator to any algorithm of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\n",
    "result=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoost is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "Here too the weak learner is a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\n",
    "result=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Gradient Boosting is:',result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "xgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\n",
    "result=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for XGBoost is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbb=xgboost.fit(X,Y)\n",
    "pred=xgbb.predict(test_data)\n",
    "test_Survived = pd.Series(pred, name=\"Survived\")\n",
    "results = pd.concat([test_y,test_Survived],axis=1)\n",
    "results.to_csv(\"titanic_xgb_predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=list(range(100,1100,100))\n",
    "learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\n",
    "gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True,n_jobs=-1)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\n",
    "result=cross_val_predict(ada,X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model=ada.fit(train_X,train_Y)\n",
    "pred=ada_model.predict(test_data)\n",
    "test_Survived = pd.Series(pred, name=\"Survived\")\n",
    "results = pd.concat([test_y,test_Survived],axis=1)\n",
    "results.to_csv(\"titanic_predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,2,figsize=(15,12))\n",
    "model=RandomForestClassifier(n_estimators=500,random_state=0)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\n",
    "ax[0,0].set_title('Feature Importance in Random Forests')\n",
    "model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\n",
    "ax[0,1].set_title('Feature Importance in AdaBoost')\n",
    "model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\n",
    "ax[1,0].set_title('Feature Importance in Gradient Boosting')\n",
    "model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\n",
    "ax[1,1].set_title('Feature Importance in XgBoost')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
